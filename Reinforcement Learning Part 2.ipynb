{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc636427-1675-4ea8-a27a-c3e0b6c1dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72f9f11-c7f7-4133-b8ac-da4a3c19470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length=10000):\n",
    "        self._max_length = max_length\n",
    "        self._buffer = []\n",
    "        self._index = 0\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        if len(self._buffer) < self._max_length:\n",
    "            self._buffer.append(experience)\n",
    "        else:\n",
    "            if self._index >= self._max_length:\n",
    "                self._index = 0\n",
    "            self._buffer[self._index] = experience\n",
    "            self._index += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self._buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b826f8-5d61-4408-b364-a1462bdd0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Sequential):\n",
    "    def __init__(self, hidden_dims):\n",
    "        for dim in hidden_dims:            \n",
    "            self.add(tf.keras.layers.Dense(dim, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5be79-d530-4088-a967-94b08db53cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add some dense layers with ReLU activations\n",
    "        self.mlp = MLP(hidden_dims)\n",
    "\n",
    "        # Add a layer for the output value\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, state, action):\n",
    "        x = tf.concat((state, action), 1)\n",
    "        x = self.mlp(x)\n",
    "        return self.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bab839-8338-4556-b243-9d29e06dd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, action_space):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add some dense layers with ReLU activations\n",
    "        self.mlp = MLP(hidden_dims)\n",
    "\n",
    "        # Add a layer for the output action\n",
    "        self.action = tf.keras.layers.Dense(action_space.shape, activation='tanh')\n",
    "        \n",
    "        self.scale = (action_space.high - action_space.low) / 2\n",
    "        self.shift = (action_space.high + action_space.low) / 2\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = self.action(x)\n",
    "        x *= self.scale\n",
    "        x += self.shift\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f3515-3ed2-4768-9ade-4cb0b77d7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, env, hidden_dims, discount=0.99, tau=1e-2):\n",
    "        self.critic = CriticNetwork(hidden_dims)\n",
    "        self.actor = ActorNetwork(hidden_dims, env.action_space)\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.target_critic = CriticNetwork(hidden_dims)\n",
    "        self.target_actor = ActorNetwork(hidden_dims, env.action_space)\n",
    "        \n",
    "        # Create optimizers\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "        # Parameters for minibatching\n",
    "        self.batch_size = 64\n",
    "    \n",
    "    def action(self, state):\n",
    "        return self.actor(state).numpy()[0]\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "\n",
    "        # Critic update\n",
    "        with tf.GradientTape() as tape:\n",
    "            target = reward + self.discount * tf.stop_gradient(self.critic(next_state, self.actor(next_state))) * (1 - done)\n",
    "            td_error = self.critic(state, action) - target\n",
    "            loss = tf.reduce_mean(td_error**2, 0)\n",
    "            \n",
    "        # Get gradients and apply them to the model's parameters\n",
    "        grads = tape.gradient(loss, self.critic.trainable_weights)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_weights))\n",
    "        \n",
    "        # Actor update\n",
    "        with tf.GradientTape() as tape:\n",
    "            action = self.actor(state)\n",
    "            value = self.critic(state, action)\n",
    "            loss = -value\n",
    "            \n",
    "        # Get gradients and apply them to the model's parameters\n",
    "        grads = tape.gradient(loss, self.actor.trainable_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_weights))\n",
    "        \n",
    "        self.update_targets()\n",
    "    \n",
    "    def update_targets(self):\n",
    "        for var, target_var in zip(self.critic.trainable_variables, self.target_critic.trainable_variables):\n",
    "            target_var.assign(var * self.tau + target_var * (1 - self.tau))\n",
    "        \n",
    "        for var, target_var in zip(self.actor.trainable_variables, self.target_actor.trainable_variables):\n",
    "            target_var.assign(var * self.tau + target_var * (1 - self.tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736bda24-4f4d-4651-bab0-e9f6bd1dca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ac4011-496e-4f22-9304-10ab744fcb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.low"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
