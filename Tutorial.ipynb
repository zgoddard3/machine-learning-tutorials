{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This notebook will walk though several exercises using tensorflow to implement a range of machine learning algorithms. In particular this will cover supervised learning and reinforcement learning algorithms which use neural networks.\n",
    "\n",
    "To start, make sure you have all dependencies install by running `pip install -r requirements.txt` from the directory containing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST\n",
    "\n",
    "We'll using a simple classification problem as an example to introduce how to create, train, and evaluate a model. The next cell will load the MNIST dataset which contains images of handwritten numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255, x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model\n",
    "\n",
    "We'll walkthrough two methods of creating a model. The first is to use Keras' Sequential model. This is convenient for quickly creating and training a model for basic supervised learning and some reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.08526918 0.08805387 0.08779302 0.06259194 0.13711196 0.11185778\n  0.13748164 0.13741958 0.06497267 0.08744836]]\n(60000, 28, 28)\n"
    }
   ],
   "source": [
    "# Create sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a flatten layer to turn MNIST images into a vector\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Add some dense layers with ReLU activations\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Add a layer for the scores of each class. Note that we leave the activation linear.\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "# Add an output layer for the softmax predictions\n",
    "model.add(tf.keras.layers.Softmax())\n",
    "\n",
    "# Test this on a sample from the dataset\n",
    "prediction = model(x_train[:1]).numpy()\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras' Model class (from which Sequential inherits), provides a convenient function for fitting data. First we must compile the model with an optimizer and loss function. We'll use the Adam optimizer and sparse categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0956 - accuracy: 0.9787\nEpoch 2/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0558 - accuracy: 0.9841\nEpoch 3/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0421 - accuracy: 0.9867\nEpoch 4/5\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0330 - accuracy: 0.9894\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0256 - accuracy: 0.9911\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1bbe313aa90>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "313/313 - 1s - loss: 0.1123 - accuracy: 0.9723\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.11232912540435791, 0.9722999930381775]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the alternative way of creating and training a model. We can create a model by creating a custom class which inherits from Keras' Model class. This method is more involved than the previous method; however, it allows for more customization. The ability to customize is often critical to ML research. Simply running a model on a new problem is usually not enough to constitute a novel contribution. You're contributions will likely come in the form of new model architectures, loss functions, optimization strategies, etc. All of these will be easier to implement if you understand how to build the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add a flatten layer to turn MNIST images into a vector\n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=(28,28))\n",
    "\n",
    "        # Add some dense layers with ReLU activations\n",
    "        self.hidden_layer1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.hidden_layer2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "\n",
    "        # Add a layer for the scores of each class. Note that we leave the activation linear.\n",
    "        self.score = tf.keras.layers.Dense(10)\n",
    "\n",
    "        # Add an output layer for the softmax predictions\n",
    "        self.probability = tf.keras.layers.Softmax()\n",
    "\n",
    "        # Create an optimizer and loss function for training\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        # Parameters for minibatching\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # Metrics\n",
    "        self.loss_metric = tf.keras.metrics.Mean()\n",
    "        self.accuracy_metric = tf.keras.metrics.Accuracy()\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Create a call function to pass input through each layer\n",
    "        # IMPORTANT NOTE: Always implement this method as 'call', don't override the '__call__' method\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.score(x)\n",
    "        return self.probability(x)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        N = y.shape[0]\n",
    "        i = 0\n",
    "\n",
    "        # Break up the data into minibatches\n",
    "        while i < N:\n",
    "            x_batch = x[i:min(i+self.batch_size, N)]\n",
    "            y_batch = y[i:min(i+self.batch_size, N)]\n",
    "\n",
    "            # This line lets us record gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.loss_fn(y_batch, y_pred)\n",
    "            \n",
    "            grads = tape.gradient(loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "            self.loss_metric(loss)\n",
    "            self.accuracy_metric(y_batch, np.argmax(y_pred.numpy(), axis=1))\n",
    "\n",
    "            i += self.batch_size\n",
    "\n",
    "            if i % (self.batch_size * 100) == 0:\n",
    "                print(\"Step {}: Average loss = {}, Accuracy = {}\".format(i/self.batch_size, self.loss_metric.result(), self.accuracy_metric.result()))\n",
    "                self.loss_metric.reset_state()\n",
    "                self.accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch:  0\nStep 100.0: Average loss = 0.7794746160507202, Accuracy = 0.7806249856948853\nStep 200.0: Average loss = 0.3627699911594391, Accuracy = 0.8951562643051147\nStep 300.0: Average loss = 0.2991768419742584, Accuracy = 0.9098437428474426\nStep 400.0: Average loss = 0.23929233849048615, Accuracy = 0.9298437237739563\nStep 500.0: Average loss = 0.2374669462442398, Accuracy = 0.9312499761581421\nStep 600.0: Average loss = 0.20516353845596313, Accuracy = 0.9417187571525574\nStep 700.0: Average loss = 0.20398493111133575, Accuracy = 0.9417187571525574\nStep 800.0: Average loss = 0.20411214232444763, Accuracy = 0.9393749833106995\nStep 900.0: Average loss = 0.16094501316547394, Accuracy = 0.9515625238418579\nEpoch:  1\nStep 100.0: Average loss = 0.12226498872041702, Accuracy = 0.965113639831543\nStep 200.0: Average loss = 0.15222826600074768, Accuracy = 0.9532812237739563\nStep 300.0: Average loss = 0.12604905664920807, Accuracy = 0.9623437523841858\nStep 400.0: Average loss = 0.11800150573253632, Accuracy = 0.9643750190734863\nStep 500.0: Average loss = 0.12300719320774078, Accuracy = 0.9642187356948853\nStep 600.0: Average loss = 0.11544680595397949, Accuracy = 0.9659374952316284\nStep 700.0: Average loss = 0.1199277937412262, Accuracy = 0.9660937786102295\nStep 800.0: Average loss = 0.12112967669963837, Accuracy = 0.964062511920929\nStep 900.0: Average loss = 0.09419288486242294, Accuracy = 0.9701562523841858\nEpoch:  2\nStep 100.0: Average loss = 0.0785343199968338, Accuracy = 0.9767045378684998\nStep 200.0: Average loss = 0.09649860113859177, Accuracy = 0.9681249856948853\nStep 300.0: Average loss = 0.08184944093227386, Accuracy = 0.9731249809265137\nStep 400.0: Average loss = 0.07984864711761475, Accuracy = 0.9754687547683716\nStep 500.0: Average loss = 0.08285264670848846, Accuracy = 0.9782812595367432\nStep 600.0: Average loss = 0.08118516951799393, Accuracy = 0.9765625\nStep 700.0: Average loss = 0.08495521545410156, Accuracy = 0.9759374856948853\nStep 800.0: Average loss = 0.08642534911632538, Accuracy = 0.9725000262260437\nStep 900.0: Average loss = 0.06306841224431992, Accuracy = 0.9803125262260437\nEpoch:  3\nStep 100.0: Average loss = 0.05540955066680908, Accuracy = 0.9835227131843567\nStep 200.0: Average loss = 0.06915011256933212, Accuracy = 0.9779687523841858\nStep 300.0: Average loss = 0.05507327616214752, Accuracy = 0.9832812547683716\nStep 400.0: Average loss = 0.05868494510650635, Accuracy = 0.9817187786102295\nStep 500.0: Average loss = 0.061486851423978806, Accuracy = 0.9821875095367432\nStep 600.0: Average loss = 0.06181713193655014, Accuracy = 0.9815624952316284\nStep 700.0: Average loss = 0.06326427310705185, Accuracy = 0.9826562404632568\nStep 800.0: Average loss = 0.0640907883644104, Accuracy = 0.9782812595367432\nStep 900.0: Average loss = 0.04482366517186165, Accuracy = 0.9867187738418579\nEpoch:  4\nStep 100.0: Average loss = 0.04149562492966652, Accuracy = 0.9887499809265137\nStep 200.0: Average loss = 0.053915489464998245, Accuracy = 0.9842187762260437\nStep 300.0: Average loss = 0.038711145520210266, Accuracy = 0.9881250262260437\nStep 400.0: Average loss = 0.04156869277358055, Accuracy = 0.9864062666893005\nStep 500.0: Average loss = 0.0453956164419651, Accuracy = 0.9876562356948853\nStep 600.0: Average loss = 0.05227290466427803, Accuracy = 0.983593761920929\nStep 700.0: Average loss = 0.04822193458676338, Accuracy = 0.9860937595367432\nStep 800.0: Average loss = 0.04470256716012955, Accuracy = 0.9856250286102295\nStep 900.0: Average loss = 0.0352865494787693, Accuracy = 0.9900000095367432\n"
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    my_model.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}