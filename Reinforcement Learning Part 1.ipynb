{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e088a871",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "This tutorial will walk through some initial concepts and algorithms for reinforcement learning (RL). I'll assume you've already done the general introduction on supervised learning with Tensorflow. We'll start with basic Q-learning to introduce the concept of RL, but you will need to be familiar with Tensorflow models for some of the later algorithms.\n",
    "\n",
    "Additionally, the focus of this tutorial will be on RL in control applications. We'll start by introducing some basic concepts followed by an example with the pendulum problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f785ff6-2916-4f2d-a998-4c28e297f991",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "Reinfocement learning problems seek to find a solution to an MDP which maximizes a reward. An MDP is represented by a set of states $S$, a set of actions $A$, a reward function $R(s_t,a_t,s_{t+1}) : S \\times A \\times S \\rightarrow \\mathbb{R}$, and a transition function $P(s_t,a_t,s_{t+1}) : S \\times A \\times S \\rightarrow [0,1]$. The reward function maps a transition $(s_t,a_t,s_{t+1})$ to a scalar reward. In many cases this may be simplified to a function of state-action pairs $(s_t,a_t)$ or just state. The transition function returns the probability that a state-action pair $(s_t,a_t)$ will result in a given state $s_{t+1}$. A solution to an MDP is a policy $\\pi(a_t|s_t) : A \\times S \\rightarrow \\mathbb{R}$ which gives the probability of an action $a_t$ given the current state $s_t$. For deterministic policies this may also be written as $\\mu(s_t) : S \\rightarrow A$ which returns an action given the current state.\n",
    "\n",
    "MDPs often have a discount factor $0 \\leq \\gamma < 1$ to weight immediate rewards more favorably than future rewards. In this case the objective of the policy is to maximize the return of a trajectory $\\tau = \\{ (s_0, a_0), (s_1, a_1), ..., (s_T, a_T)\\}$. The return is defined as $G = \\sum_{t=0}^{T-1} \\gamma^t R(s_t, a_t, s_{t+1})$.\n",
    "\n",
    "## Control Theory\n",
    "\n",
    "Control problems can be modeled as an MDP with some minor tweaks and typically different notation. For notation, the state space is often denoted as $X$ and the action space is called the control space denoted as $U$. The reward function is also typically replaced with a cost function $J(x,u)$ which we wish to minimize. The transition function is analogous to the system dynamics $\\dot{x} = f(x,u,w,t)$. We'll mostly be working with deterministic, time-invariant systems represented by the simplified expression $\\dot{x} = f(x,u)$. The following notebook will be dealing with simulations and numerical algorithms, so it's also convenient to work with a discrete-time representation $x_{t+1} = f(x_t, u_t)$. In this representation the parallels to MDPs should be fairly obvious. \n",
    "\n",
    "## Value Iteration\n",
    "\n",
    "Value iteration is one algorithm for solving MDPs if the transition and reward functions are known. Understanding how it works will provide some insight into the RL algorithms we discuss later, which have no direct access to the transition or reward function. Value iteration is based on the principal of optimality. In simple terms, this states that the tail end of any optimal trajectory is also optimal. In more mathematical terms, if you have an optimal trajectory from $x_0$ to $x_T$ which passes through $x_t$ then the portion of that trajectory from $x_t$ to $x_T$ is the optimal solution from $x_t$. This is the basic for dynamic programming methods, which typically find optimal solutions by working backwards from the goal finding optimal solutions to subproblems. Value iteration does this by iteratively solving the following equation:\n",
    "\n",
    "$V_{i+1}(s_t) = \\max_{a_t} \\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})\\left[R(s_t, a_t, s_{t+1}) + \\gamma V_i(s_{t+1})\\right]$\n",
    "\n",
    "In the discrete case, this is guaranteed to converge to an optimal solution $V^*$ which has the following property:\n",
    "\n",
    "$V^*(s_t) = \\max_{a_t} \\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})\\left[R(s_t, a_t, s_{t+1}) + \\gamma V^*(s_{t+1})\\right]$\n",
    "\n",
    "The optimal policy for the MDP is given by:\n",
    "\n",
    "$\\pi^*(s_t) = \\arg\\max_{a_t} \\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})\\left[R(s_t, a_t, s_{t+1}) + \\gamma V^*(s_{t+1})\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f0e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22390a12",
   "metadata": {},
   "source": [
    "## Loading Gym Environments\n",
    "\n",
    "OpenAI's Gym has a range of environments designed for RL experiments load in and use out-of-the-box. These environments include a simple cartpole problem. This environment gives the position and velocity of the cart and the angle and angular rate of the pendulum as the observation. There are two possible actions for the agent: push left or push right. The agent receives a reward of +1 every time step until the cart position or pendulum angle pass a given threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0ea1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: Box(-8.0, 8.0, (3,), float32)\n",
      "Action: Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(\"Observation:\", env.observation_space)\n",
    "print(\"Action:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119804db",
   "metadata": {},
   "source": [
    "We can quickly visualize this environment to see how an agent is performing. Since we don't yet have an agent, we'll just sample actions randomly. Of course, this policy won't last long before failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e78bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300118de",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "We start by solving this environment with the Q-learning algorithm. This isn't a *Deep* RL algorithm, but it provides a more clear example of the principles that later algorithms will be working off of. \n",
    "\n",
    "The objective of Q-Learning (and many algorithms we explore later) is to learn a q-function which $Q : S \\times A \\rightarrow \\mathbb{R}$ predicts the value for taking an action at a given state. This is similar to the value function in Value Iteration, but determines the value for state-action pairs instead of state alone. Similarly to value iteration, the optimal q-function should satisfy the following condition:\n",
    "\n",
    "$Q^*(s_t, a_t) = \\sum_{s_{t+1}} P(s_t, a_t, s_{t+t}) \\left[R(s_t, a_t, s_{t+1}) + \\gamma \\max_{a_{t+1}} Q^*(s_{t+1}, a_{t+1})\\right]$\n",
    "\n",
    "Regular Q-Learning represents this function as a look-up table and learns each value through dynamic programming. Since we don't have access to the transition function or reward function, we have to learn this function by sampling experience. Experience is sampled from the environment as transitions in the form $(s_t, a_t, s_{t+1})$. Each time new experience is sampled, the Q value is updated by the equation:\n",
    "\n",
    "$Q(s_t, a_t) \\leftarrow (1 - \\alpha) Q(s_t, a_t) + \\alpha \\left[R(s_t, a_t, s_{t+1}) + \\gamma \\max_a Q(s_{t+1}, a)\\right]$\n",
    "\n",
    "The learning rate $\\alpha$ is typically a small value which can be adjusted to change how fast the q-function is updated. This soft update allows the q-function to approximate stochastic transitions. The pendulum environment is deterministic, but we need to discretize the state space to create a tabular q-function. The soft update helps since our discretized state space doesn't perfectly represent the actual state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b420b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle Break Points: [-3.14159265 -2.82743339 -2.51327412 -2.19911486 -1.88495559 -1.57079633\n",
      " -1.25663706 -0.9424778  -0.62831853 -0.31415927  0.          0.31415927\n",
      "  0.62831853  0.9424778   1.25663706  1.57079633  1.88495559  2.19911486\n",
      "  2.51327412  2.82743339  3.14159265]\n",
      "Angular Rate Break Points: [-inf -7.5 -7.  -6.5 -6.  -5.5 -5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5\n",
      " -1.  -0.5  0.   0.5  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5\n",
      "  6.   6.5  7.   7.5]\n",
      "Number of States: 672\n"
     ]
    }
   ],
   "source": [
    "theta = np.linspace(-np.pi, np.pi, 21)\n",
    "thetadot = np.arange(-8, 7.6, .5)\n",
    "thetadot[0] = -np.inf # Setting first entry to neg. infinity for numerical errors\n",
    "\n",
    "print(\"Angle Break Points:\", theta)\n",
    "print(\"Angular Rate Break Points:\", thetadot)\n",
    "\n",
    "N = theta.shape[0]*thetadot.shape[0]\n",
    "\n",
    "print(\"Number of States:\", N)\n",
    "\n",
    "Q = np.zeros((N,3))\n",
    "\n",
    "\n",
    "# Maps the continuous angle and angular rate to a discrete state value\n",
    "def state_map(obs):\n",
    "    angle = np.arctan2(obs[1], obs[0])\n",
    "    i = np.nonzero(theta <= angle)[0][-1]\n",
    "    j = np.nonzero(thetadot <= obs[2])[0][-1]\n",
    "    \n",
    "    return j + i*thetadot.shape[0]\n",
    "\n",
    "# Maps the action index to a continuous action\n",
    "def action_map(action):\n",
    "    if action == 0:\n",
    "        return np.array([-2])\n",
    "    elif action == 1:\n",
    "        return np.array([0])\n",
    "    else:\n",
    "        return np.array([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c4dfc",
   "metadata": {},
   "source": [
    "### Training a Q Table\n",
    "\n",
    "Now we just need to collect some experience and update our Q function until it converges on a successful policy. When you run the following cell it will show you the average return for the last 100 episodes. You can use this to gauge how well the policy is doing. If you run the cell multiple times it will continue training the Q-table from where it left off. You may need to run it a few times before it settles on a final value. The average return should settle around -400 to -500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38c3eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 999: Running Average = -486.08309975314376\r"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "returns = []\n",
    "for i in range(1000):\n",
    "    obs = env.reset()\n",
    "    total_return = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        ind = state_map(obs)\n",
    "        if np.random.rand() > .2:\n",
    "            action = np.argmax(Q[ind])\n",
    "        else:\n",
    "            action = np.random.randint(3)\n",
    "        obs, reward, done, _ = env.step(action_map(action))\n",
    "        total_return += reward\n",
    "\n",
    "        Q[ind, action] = (1 - lr) * Q[ind, action] + lr * (reward + 0.99 * np.max(Q[state_map(obs)]))\n",
    "    \n",
    "    returns.append(total_return)\n",
    "    if len(returns) > 100:\n",
    "        returns.pop(0)\n",
    "    print(\"Episode {}: Running Average = {}\".format(i, sum(returns)/len(returns)), flush=True, end='\\r')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba164e4f",
   "metadata": {},
   "source": [
    "### Visualizing the Final Policy\n",
    "\n",
    "Now that we've trained a Q function we can visualize how well it performs. It should be able to swing the pendulum up and hold it mostly upright. It probably won't be perfect for every initialization. Try going back to the training loop and adjusting the learning rate to get a slightly better performance. You should be able to get it to settle around an average return of -300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a07d0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "done = False\n",
    "while not done:\n",
    "    ind = state_map(obs)\n",
    "    action = np.argmax(Q[ind])\n",
    "    obs, reward, done, _ = env.step(action_map(action))\n",
    "\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a9dba-69a8-4d9c-9b3d-3c86d1ba3487",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "Now we'll move on to the actual *Deep* Learning part. The table does alright for the pendulum, but the number of states you need to store will explode with higher dimensional state spaces and finer resolutions. It would be better if we can store the Q value as a function of the state instead of a look up table. We'll need a function approximator, which could be a few different things such as a linear model or interpolator. The popular choice, and arguably most powerful, is a deep neural network which is also where the following methods get the name *Deep* Reinforcement Learning. You'll want to complete the Introduction notebook before continuing if you haven't already.\n",
    "\n",
    "The first algorithm we'll look at is the Deep Q Network. This method uses a deep neural network to estimate Q values for a discrete set of actions given any point in a continuous state space. We'll denote the Q function represented by this network as $Q_\\theta (s, a)$ where $\\theta$ is the set of parameters for the network. We have to update these parameters in a way that moves the Q function towards the target value at the sampled state. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
