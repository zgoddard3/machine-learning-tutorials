{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e088a871",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "This tutorial will walk through some initial concepts and algorithms for reinforcement learning (RL). I'll assume you've already done the general introduction on supervised learning with Tensorflow. We'll start with basic Q-learning to introduce the concept of RL, but you will need to be familiar with Tensorflow models for some of the later algorithms.\n",
    "\n",
    "Additionally, the focus of this tutorial will be on RL in control applications. We'll start by introducing some basic concepts followed by an example with the pendulum problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f785ff6-2916-4f2d-a998-4c28e297f991",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "Reinfocement learning problems seek to find a solution to an MDP which maximizes a reward. An MDP is represented by a set of states $S$, a set of actions $A$, a reward function $R(s_t,a_t,s_{t+1}) : S \\times A \\times S \\rightarrow \\mathbb{R}$, and a transition function $P(s_t,a_t,s_{t+1}) : S \\times A \\times S \\rightarrow [0,1]$. The reward function maps a transition $(s_t,a_t,s_{t+1})$ to a scalar reward. In many cases this may be simplified to a function of state-action pairs $(s_t,a_t)$ or just state. The transition function returns the probability that a state-action pair $(s_t,a_t)$ will result in a given state $s_{t+1}$. A solution to an MDP is a policy $\\pi(a_t|s_t) : A \\times S \\rightarrow \\mathbb{R}$ which gives the probability of an action $a_t$ given the current state $s_t$. For deterministic policies this may also be written as $\\mu(s_t) : S \\rightarrow A$ which returns an action given the current state.\n",
    "\n",
    "MDPs often have a discount factor $0 \\leq \\gamma < 1$ to weight immediate rewards more favorably than future rewards. In this case the objective of the policy is to maximize the return of a trajectory $\\tau = \\{ (s_0, a_0), (s_1, a_1), ..., (s_T, a_T)\\}$. The return is defined as $G = \\sum_{t=0}^{T-1} \\gamma^t R(s_t, a_t, s_{t+1})$.\n",
    "\n",
    "## Control Theory\n",
    "\n",
    "Control problems can usually be modeled as an MDP with some minor tweaks and typically different notation. For notation, the state space is often denoted as $X$ and the action space is called the control space denoted as $U$. The reward function is also typically replaced with a cost function $J(x,u)$ which we wish to minimize. The transition function is analogous to the system dynamics $\\dot{x} = f(x,u,w,t)$. We'll mostly be working with deterministic, time-invariant systems represented by the simplified expression $\\dot{x} = f(x,u)$. The following notebook will be dealing with simulations and numerical algorithms, so it's also convenient to work with a discrete-time representation $x_{t+1} = f(x_t, u_t)$. In this representation the parallels to MDPs should be fairly obvious. \n",
    "\n",
    "## Value Iteration\n",
    "\n",
    "Value iteration is one algorithm for solving MDPs if the transition and reward functions are known. Understanding how it works will provide some insight into the RL algorithms we discuss later, which have no direct access to the transition or reward function. Value iteration is based on the principal of optimality. In simple terms, this states that the tail end of any optimal trajectory is also optimal. In more mathematical terms, if you have an optimal trajectory from $x_0$ to $x_T$ which passes through $x_t$ then the portion of that trajectory from $x_t$ to $x_T$ is the optimal solution from $x_t$. This is the basic for dynamic programming methods, which typically find optimal solutions by working backwards from the goal finding optimal solutions to subproblems. Value iteration does this by iteratively solving the following equation:\n",
    "\n",
    "$V_{i+1}(s_t) = \\max_{a_t} \\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})\\left[R(s_t, a_t, s_{t+1}) + \\gamma V_i(s_{t+1})\\right]$\n",
    "\n",
    "In the discrete case, this is guaranteed to converge to an optimal solution $V^*$ which has the following property:\n",
    "\n",
    "$V^*(s_t) = \\max_{a_t} \\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})\\left[R(s_t, a_t, s_{t+1}) + \\gamma V^*(s_{t+1})\\right]$\n",
    "\n",
    "The optimal policy for the MDP is given by:\n",
    "\n",
    "$\\pi^*(s_t) = \\arg\\max_{a_t} \\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})\\left[R(s_t, a_t, s_{t+1}) + \\gamma V^*(s_{t+1})\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f0e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22390a12",
   "metadata": {},
   "source": [
    "## Loading Gym Environments\n",
    "\n",
    "OpenAI's Gym has a range of environments designed for RL experiments load in and use out-of-the-box. These environments include a simple cartpole problem. This environment gives the position and velocity of the cart and the angle and angular rate of the pendulum as the observation. There are two possible actions for the agent: push left or push right. The agent receives a reward of +1 every time step until the cart position or pendulum angle pass a given threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0ea1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "Action: Box([-2.], [2.], (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(\"Observation:\", env.observation_space)\n",
    "print(\"Action:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119804db",
   "metadata": {},
   "source": [
    "We can quickly visualize this environment to see how an agent is performing. Since we don't yet have an agent, we'll just sample actions randomly. Of course, this policy won't last long before failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e78bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300118de",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "We start by solving this environment with the Q-learning algorithm. This isn't a *Deep* RL algorithm, but it provides a more clear example of the principles that later algorithms will be working off of. \n",
    "\n",
    "The objective of Q-Learning (and many algorithms we explore later) is to learn a q-function which $Q : S \\times A \\rightarrow \\mathbb{R}$ predicts the value for taking an action at a given state. This is similar to the value function in Value Iteration, but determines the value for state-action pairs instead of state alone. Similarly to value iteration, the optimal q-function should satisfy the following condition:\n",
    "\n",
    "$Q^*(s_t, a_t) = \\sum_{s_{t+1}} P(s_t, a_t, s_{t+t}) \\left[R(s_t, a_t, s_{t+1}) + \\gamma \\max_{a_{t+1}} Q^*(s_{t+1}, a_{t+1})\\right]$\n",
    "\n",
    "Regular Q-Learning represents this function as a look-up table and learns each value through dynamic programming. Since we don't have access to the transition function or reward function, we have to learn this function by sampling experience. Experience is sampled from the environment as transitions in the form $(s_t, a_t, s_{t+1})$. Each time new experience is sampled, the Q value is updated by the equation:\n",
    "\n",
    "$Q(s_t, a_t) \\leftarrow (1 - \\alpha) Q(s_t, a_t) + \\alpha \\left[R(s_t, a_t, s_{t+1}) + \\gamma \\max_a Q(s_{t+1}, a)\\right]$\n",
    "\n",
    "The learning rate $\\alpha$ is typically a small value which can be adjusted to change how fast the q-function is updated. This soft update allows the q-function to approximate stochastic transitions. The pendulum environment is deterministic, but we need to discretize the state space to create a tabular q-function. The soft update helps since our discretized state space doesn't perfectly represent the actual state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b420b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [-3.14159265 -2.82743339 -2.51327412 -2.19911486 -1.88495559 -1.57079633\n",
      " -1.25663706 -0.9424778  -0.62831853 -0.31415927  0.          0.31415927\n",
      "  0.62831853  0.9424778   1.25663706  1.57079633  1.88495559  2.19911486\n",
      "  2.51327412  2.82743339  3.14159265]\n",
      "thetadot: [-inf -7.5 -7.  -6.5 -6.  -5.5 -5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5\n",
      " -1.  -0.5  0.   0.5  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5\n",
      "  6.   6.5  7.   7.5]\n",
      "N: 672\n"
     ]
    }
   ],
   "source": [
    "theta = np.linspace(-np.pi, np.pi, 21)\n",
    "thetadot = np.arange(-8, 7.6, .5)\n",
    "thetadot[0] = -np.inf # Setting first entry to neg. infinity for numerical errors\n",
    "\n",
    "print(\"theta:\", theta)\n",
    "print(\"thetadot:\", thetadot)\n",
    "\n",
    "N = theta.shape[0]*thetadot.shape[0]\n",
    "\n",
    "print(\"N:\", N)\n",
    "\n",
    "Q = np.zeros((N,3))\n",
    "\n",
    "def state_map(obs):\n",
    "    angle = np.arctan2(obs[1], obs[0])\n",
    "    i = np.nonzero(theta <= angle)[0][-1]\n",
    "    j = np.nonzero(thetadot <= obs[2])[0][-1]\n",
    "    \n",
    "    return j + i*thetadot.shape[0]\n",
    "\n",
    "def action_map(action):\n",
    "    if action == 0:\n",
    "        return np.array([-2])\n",
    "    elif action == 1:\n",
    "        return np.array([0])\n",
    "    else:\n",
    "        return np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9f8e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "641\n",
      "[-2]\n",
      "[0]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(state_map([-1, -0.001, -8]))\n",
    "print(state_map([np.cos(-2.6), np.sin(-2.6), -8]))\n",
    "print(state_map([-1, 0, -7.4]))\n",
    "\n",
    "print(action_map(0))\n",
    "print(action_map(1))\n",
    "print(action_map(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c4dfc",
   "metadata": {},
   "source": [
    "Now we just need to collect some experience and update our Q function until it converges on a successful policy. Feel free to run the following cell a few times until the total return is at a satisfactory value. (Note this probably won't acheive a particularly high score. Just train it until it can reach a score of 100 every once in a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a38c3eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Total return: -381.5415588095833\n",
      "Episode 1\n",
      "Total return: -623.8190069295432\n",
      "Episode 2\n",
      "Total return: -748.0620876034568\n",
      "Episode 3\n",
      "Total return: -374.5873348590019\n",
      "Episode 4\n",
      "Total return: -635.6661592066587\n",
      "Episode 5\n",
      "Total return: -744.8051013099672\n",
      "Episode 6\n",
      "Total return: -126.36726749240648\n",
      "Episode 7\n",
      "Total return: -608.4058701534519\n",
      "Episode 8\n",
      "Total return: -490.4911026004222\n",
      "Episode 9\n",
      "Total return: -368.14152708182047\n",
      "Episode 10\n",
      "Total return: -251.4697137676263\n",
      "Episode 11\n",
      "Total return: -616.7708352909402\n",
      "Episode 12\n",
      "Total return: -648.3177695298467\n",
      "Episode 13\n",
      "Total return: -379.73448745500156\n",
      "Episode 14\n",
      "Total return: -380.09451831415697\n",
      "Episode 15\n",
      "Total return: -237.72955350357745\n",
      "Episode 16\n",
      "Total return: -824.0895109426322\n",
      "Episode 17\n",
      "Total return: -254.2110737472445\n",
      "Episode 18\n",
      "Total return: -243.19367181234455\n",
      "Episode 19\n",
      "Total return: -451.78642946398935\n",
      "Episode 20\n",
      "Total return: -635.1523562833295\n",
      "Episode 21\n",
      "Total return: -497.9901153673096\n",
      "Episode 22\n",
      "Total return: -508.9569554862897\n",
      "Episode 23\n",
      "Total return: -502.09442331906587\n",
      "Episode 24\n",
      "Total return: -766.8055700610571\n",
      "Episode 25\n",
      "Total return: -614.7960090589852\n",
      "Episode 26\n",
      "Total return: -491.86585364703603\n",
      "Episode 27\n",
      "Total return: -371.3748891468128\n",
      "Episode 28\n",
      "Total return: -750.7823046292795\n",
      "Episode 29\n",
      "Total return: -127.83931327136924\n",
      "Episode 30\n",
      "Total return: -2.7740690900525147\n",
      "Episode 31\n",
      "Total return: -605.6839804619417\n",
      "Episode 32\n",
      "Total return: -362.53950033422996\n",
      "Episode 33\n",
      "Total return: -126.70610854886178\n",
      "Episode 34\n",
      "Total return: -384.9915568728327\n",
      "Episode 35\n",
      "Total return: -382.38535207383904\n",
      "Episode 36\n",
      "Total return: -628.2467240959367\n",
      "Episode 37\n",
      "Total return: -737.0247862037919\n",
      "Episode 38\n",
      "Total return: -370.26305465055896\n",
      "Episode 39\n",
      "Total return: -730.3997700443406\n",
      "Episode 40\n",
      "Total return: -623.6171340809877\n",
      "Episode 41\n",
      "Total return: -512.1646991342093\n",
      "Episode 42\n",
      "Total return: -614.1279077405777\n",
      "Episode 43\n",
      "Total return: -621.1292893234512\n",
      "Episode 44\n",
      "Total return: -370.9013891539108\n",
      "Episode 45\n",
      "Total return: -249.20556509512537\n",
      "Episode 46\n",
      "Total return: -622.7450042328059\n",
      "Episode 47\n",
      "Total return: -373.17703339501503\n",
      "Episode 48\n",
      "Total return: -505.32609068868703\n",
      "Episode 49\n",
      "Total return: -250.73072290991894\n",
      "Episode 50\n",
      "Total return: -721.707877702008\n",
      "Episode 51\n",
      "Total return: -129.39515709732956\n",
      "Episode 52\n",
      "Total return: -432.9696087574185\n",
      "Episode 53\n",
      "Total return: -252.74288136019922\n",
      "Episode 54\n",
      "Total return: -126.0672392922852\n",
      "Episode 55\n",
      "Total return: -497.90612356252547\n",
      "Episode 56\n",
      "Total return: -571.2311982229115\n",
      "Episode 57\n",
      "Total return: -620.75748205533\n",
      "Episode 58\n",
      "Total return: -379.4675000276091\n",
      "Episode 59\n",
      "Total return: -626.6120268966853\n",
      "Episode 60\n",
      "Total return: -128.21265284522113\n",
      "Episode 61\n",
      "Total return: -301.3803969182341\n",
      "Episode 62\n",
      "Total return: -359.26533712079924\n",
      "Episode 63\n",
      "Total return: -866.0844168689869\n",
      "Episode 64\n",
      "Total return: -241.99130566861652\n",
      "Episode 65\n",
      "Total return: -757.9195393168378\n",
      "Episode 66\n",
      "Total return: -250.98845513357597\n",
      "Episode 67\n",
      "Total return: -495.39080168097365\n",
      "Episode 68\n",
      "Total return: -382.44377537149705\n",
      "Episode 69\n",
      "Total return: -729.9288918811004\n",
      "Episode 70\n",
      "Total return: -525.8784683395406\n",
      "Episode 71\n",
      "Total return: -489.0871333641666\n",
      "Episode 72\n",
      "Total return: -249.14040111530326\n",
      "Episode 73\n",
      "Total return: -385.4320505010247\n",
      "Episode 74\n",
      "Total return: -262.73226880042205\n",
      "Episode 75\n",
      "Total return: -480.5276413483575\n",
      "Episode 76\n",
      "Total return: -504.02683883566914\n",
      "Episode 77\n",
      "Total return: -657.128715433747\n",
      "Episode 78\n",
      "Total return: -494.3871665157909\n",
      "Episode 79\n",
      "Total return: -733.4232760953466\n",
      "Episode 80\n",
      "Total return: -777.890671735988\n",
      "Episode 81\n",
      "Total return: -751.0158765369629\n",
      "Episode 82\n",
      "Total return: -856.4271082865116\n",
      "Episode 83\n",
      "Total return: -506.0780845380987\n",
      "Episode 84\n",
      "Total return: -613.230884346462\n",
      "Episode 85\n",
      "Total return: -246.6239664586766\n",
      "Episode 86\n",
      "Total return: -731.4443868687518\n",
      "Episode 87\n",
      "Total return: -379.8995790666568\n",
      "Episode 88\n",
      "Total return: -621.6698445281979\n",
      "Episode 89\n",
      "Total return: -625.8155451210425\n",
      "Episode 90\n",
      "Total return: -626.7889617944788\n",
      "Episode 91\n",
      "Total return: -784.9325242612556\n",
      "Episode 92\n",
      "Total return: -505.0483628697253\n",
      "Episode 93\n",
      "Total return: -741.3295234318397\n",
      "Episode 94\n",
      "Total return: -255.45588635338845\n",
      "Episode 95\n",
      "Total return: -745.6272228750763\n",
      "Episode 96\n",
      "Total return: -374.94437518074244\n",
      "Episode 97\n",
      "Total return: -501.76123523468067\n",
      "Episode 98\n",
      "Total return: -383.52578360811725\n",
      "Episode 99\n",
      "Total return: -386.6040788517074\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "for i in range(100):\n",
    "    print(\"Episode\", i)\n",
    "    obs = env.reset()\n",
    "    total_return = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        ind = state_map(obs)\n",
    "        if np.random.rand() > .2:\n",
    "            action = np.argmax(Q[ind])\n",
    "        else:\n",
    "            action = np.random.randint(3)\n",
    "        obs, reward, done, _ = env.step(action_map(action))\n",
    "        total_return += reward\n",
    "\n",
    "        Q[ind, action] = (1 - lr) * Q[ind, action] + lr * (reward + 0.99 * np.max(Q[state_map(obs)]))\n",
    "    print(\"Total return:\", total_return)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba164e4f",
   "metadata": {},
   "source": [
    "Now that we've trained a Q function we can visualize how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a07d0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "done = False\n",
    "while not done:\n",
    "    ind = state_map(obs)\n",
    "    action = np.argmax(Q[ind])\n",
    "    obs, reward, done, _ = env.step(action_map(action))\n",
    "\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
